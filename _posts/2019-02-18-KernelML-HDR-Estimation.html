---
layout: post
title:  "KernelML - High Density Region Estimation"
date: 2019-02-19 12:00:00
author: Rohan Kotwani
excerpt: "KernelML - HDRE"
tags: 
- Optimization
- KDE

---


<head>

</head>

<body>

    <p>
    KernelML is a brute force optimizer that uses parameter constraints and sampling methods to minimize a customizable loss function. The package uses a Cythonized backend and parallelizes operations across multiple cores with the IpyParallel. KernelML is now available on the Anaconda cloud and PyPi (pip).
    </p>
    
    <p>Please see the notebook for High Density Region Estimation with KernelML on the <a href="https://github.com/Freedomtowin/kernelml/blob/master/kernelml-high-density-region-estimator.ipynb">documentation page</a>.</p>

    </h1>High Density Region Estimation</h1>

    <p>
    Data scientists and predictive modelers often use 1-D and 2-D aggregate statistics for exploratory analysis, data cleaning, and feature creation. Higher dimensional aggregations, i.e., 3 dimensional and above, are more difficult to visualize and understand. High density regions are one example of these N-dimensional statistics. High density regions can be useful for summarizing common characteristics across multiple variables. Another use case is to validate a forecast prediction’s plausibility by exploring the densities associated with the forecast. Other machine learning approaches such as clustering and kernel density estimation are similar to finding high density regions, but these methods are different in a few important ways. It is worth noting why these methods, while useful, are not design exactly designed for the purpose of finding these regions. The goal is to use KernelML to efficiently find the regions of highest density for an N-dimensional dataset.</p>

    <p>My approach to developing this algorithm was to find a set of, common sense, constraints to construct the loss metric.</p>
    
    <p>The high density region estimator, HDRE, algorithm uses N multivariate uniform distributions to cluster the data. Uniform distributions are less sensitive to outliers than normal distributions, and these distribution truncate low correlation across the vertical and horizontal axes while keeping high correlations along the diagonal axes. The clusters are constrained to shared variance across all clusters and equal variance across all dimensions. The data should be normalized to allow the clusters to scale properly across each dimension. The video below shows the HDRE algorithm in action.</p>
    
    <iframe width="560" height="315" src="https://www.youtube.com/embed/W6Ey0i7ayBc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    
    <h1>HDRE with Gaussian Mixture Models</h1>
    
    <p>Clustering methods such as K-means and Gaussian mixture models, GMMs, use an iterative optimization algorithm that cycles between assigning data points to clusters and updating the cluster’s parameters. These clusters can grow in size and change shape. This property can be useful in some situations, but the goal is fundamentally different than finding regions of high density. For example, a small pocket of extreme values can significantly change the cluster solution. Gaussian mixtures are flexible because each cluster has its own unique weight, mean vector, and covariance matrix, but this flexibility makes it difficult to compare the clusters’ density. The GMM optimization algorithm can be customized to keep the weight of each cluster equal to 1/N where N is the number of clusters. This will improve the representation of the estimated density, but an observation can still be assigned to a cluster if it is dissimilar on a particular dimension as long it is similar on the rest of the dimensions. The plots below show a comparison between the modified GMM, shown in the left plot, and the HDRE algorithm, shown in the right plot, on a multivariate normal mixture dataset with random uniform noise added to it. Notice how the GMM tends to push close clusters away and include outliers into the cluster solution.</p>
    
    <p>It is possible to do a pull-request on scikit-learn’s current implementation of the Gaussian mixture model and modify the optimization procedure to keep equal weight on all clusters.</p>
    
    <figure><img src='/hdre_images/output_0.png' /> <figcaption>HDRE solution</figcaption></figure>
    
    <figure><img src='/hdre_images/output_1.png' /> <figcaption>GMM solution</figcaption></figure>

    <p>
    A cutoff threshold was found so that 80% of the data was clustered for both methods. Sklearn’s GaussianMixture class has an “_estimate_log_prob_resp” attribute that can be used to find the raw probabilities of an observation belonging to the closest cluster, i.e., the maximum exponential of the log probabilities. Notice how the Gaussian mixture solution tends to push close clusters away from each other. HDRE clusters are assigned data points only if the data points are within that cluster’s N-dimensional hyper-cube. Not all observations are within a hyper-cube cluster so a small variance-buffer can be added to the increase the size of each hyper-cube. If data point lies within multiple hyper-cube clusters, the data point can be assigned to the closest HDRE cluster, where distance is calculated using a Chebyshev distance metric.</p>
    
    <h1>Kernel Density Estimation</h1>

    <p>
    Kernel Density Estimation, KDE, is a non-parametric method used to estimate the probability density of a “random variable” at a given data point. The estimator uses a smoothing parameter, h, to estimate the local density around a data point. The size of the smoothing parameter strongly effects the density estimate result. The HDRE creates an KDE for the cluster solution and then compared it to the data’s KDE. A larger smoothing parameter allows the hyper-cube HDR cluster to cover more space in the KDE. Thus, the magnitude of the smoothing parameter is inversely proportional to the size of each HDR cluster.
    </p>
    
    <p>It is possible to estimate the points of highest density by performing a grid search of data points between the max-min of each dimension. However, this can be computationally expensive when the dimensionality of the data is high. The equation (1) shown the kernel density estimation equation.</p>

    <figure><img src='/kml_images/output_2.png' /> <figcaption></figcaption></figure>
    
    <p>The K function corresponds to a Gaussian kernel, h is the kernel's bandwidth, n is the number of data points, x is the dataset, and x_i is a grid point. The reason why using the KDE is impractical for estimating high density region can be seen in the following example: let's say we have a medium size dataset with 10,000 rows and 30 columns. If we search 15 data points between the max and min of each dimension, the density will need to be estimated for a total of 15³⁰ data points. The cost is multiplied by the, vectorized, operation(s) needed to compute the distances between each grid point and the rest of the dataset. After computing the highest points of density, a "peak clustering" algorithm would be needed to assign high density points to a region.</p>
    
    <h1>HDRE with KernelML</h1>

    <p>There are three main efficiency problems: 1) The HDRE cluster solution's and the data's KDE need to be efficiently compared in an N-dimensional space 2) and the comparisons need to be efficient with respect to the number of data points.</p>

    <p>Part 1) the HDRE cluster solution's and and the data's KDE were compared by dividing the N-dimensional space into sets of 2-D slices. The KDE was computed for those slices. The loss metric can then be aggregated for each 2-D KDE slice which reduces the computational complexity from N² to (N+1)*N/2 -N. Part 2) The 2-D KDE can be estimated with a 2-D Gaussian filtered histogram. The filter makes each KDE slice continuous and smooths out the loss function. Fast Fourier transforms can be used to compute the convolution between the Gaussian filter and the (histogram) KDE. This further decreases the computational overhead. Part 3) There are many loss metrics that can compare the similarity between 2-D KDE slices. Sum of squared errors is a good metric because it maximizes the error on large differences and minimizes the error on smaller differences. The equation below (2), summarizes the loss metric used by the HDRE algorithm.</p>
    
    <figure><img src='/kml_images/output_3.png' /> <figcaption></figcaption></figure>
    
    <p>The loss metric takes the sum of squared errors between the 2-D KDE slices for the cluster solution and the data. The KDE slices are created and compared for unique dimensional pair combinations. Because the the HDRE clusters are uniform distributions, the KDE can be easily filled in with a few data points and then L-1 normalized. The top plot, below, shows the Gaussian filtered HDRE cluster solution for the example shown in the video, in the first section. Notice how the jagged edges of the uniform distribution clusters are smoothed out. The bottom plot shows the KDE for the original, multivariate Gaussian distributed, data.</p>
    
    <h1>Final Notes</h1>

    <p>Three criteria were used to evaluate the quality of each cluster solution: 1) the overall loss metric 2) the uniformity of cluster weights 3) the similarity of the observations to the cluster's centroid. The HDR estimator was added to KernelML as an extension. If you are interested in supporting KernelML's development and would like to add your own domain specific extension, I will be adding KernelML extensions in exchange for donations! I couldn't find many resources on this particular topic. I credit the term "high density regions" to the paper below.</p>

    <p>Fadallah, A. "High Density Regions for Uni- and Bivariate Densities." https://thesis.eur.nl/pub/9449/9449-Fadallah.pdf Rotterdam, 6 July 2011</p>


</body>
